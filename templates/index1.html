<!DOCTYPE HTML>
<html>
<head>
	<title> Realtime voice chat </title>
	<script type="text/javascript" src="//code.jquery.com/jquery-1.4.2.min.js"></script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/socket.io/1.3.5/socket.io.min.js"></script>
    <script type="text/javascript" charset="utf-8">

    $(document).ready(function(){
    	namespace = '/test';

    	var socket = io.connect('http://' + document.domain + ':' + location.port + namespace);

    	socket.on('connected', function(msg){
    		$('#log').append('<br>' + $('<div/>').text('message : ' + msg.data).html());
    	});

        // socket.on('broadcast', function(stream){
        //     var audio = document.querySelector('audio');
        //     var binarydata = [];
        //     binarydata.push(stream.data);
        //     audio.src = window.URL.createObjectURL(new Blob(binarydata));
        //     audio.onloadedmetadata = function(e){
        //         audio.play();
        //     };
        //     console.log('message arrived');
        // });

        function hasGetUserMedia() {
        return !!(navigator.getUserMedia || navigator.webkitGetUserMedia ||
                navigator.mozGetUserMedia || navigator.msGetUserMedia);
        }

        if (hasGetUserMedia()) {
        // Good to go!
        } else {
            alert('getUserMedia() is not supported in your browser');
        }
    });

    </script>
</head>

<body>
    Say anything
    <h2>Received:</h2>
    <div id ="log"></div>
    <audio id = "audio" autoplay></audio>
    <script>
        'use strict';
        // Put variables in global scope to make them available to the browser console.
        // var audio = document.querySelector('audio');

        var constraints = window.constraints = {
          audio: true,
          video: false
        };

        var namespace = '/test';
        var socket = io.connect('http://' + document.domain + ':' + location.port + namespace);

        function handleSuccess(stream) {
            var audioTracks = stream.getAudioTracks();
            console.log('Got stream with constraints:', constraints);
            console.log('Using audio device: ' + audioTracks[0].label);
            console.log('stream to string : ' + stream.toString())
            stream.oninactive = function() {
            console.log('Stream ended');
            };

            window.stream = stream; // make variable available to browser console
            audio.srcObject = stream;

            var audioContext = window.AudioContext || window.webkitAudioContext;
            var context = new audioContext();
         
            // // retrieve the current sample rate to be used for WAV packaging
            // sampleRate = context.sampleRate;
         
            // // creates a gain node
            // volume = context.createGain();

            // creates an audio node from the microphone incoming stream
            var audioInput = context.createMediaStreamSource(stream);
            while(stream.active){
                console.log('active');
                socket.emit('speak', {data:stream});
            }

            // // connect the stream to the gain node
            // audioInput.connect(volume);
         
            //  From the spec: This value controls how frequently the audioprocess event is 
            // dispatched and how many sample-frames need to be processed each call. 
            // Lower values for buffer size will result in a lower (better) latency. 
            // Higher values will be necessary to avoid audio breakup and glitches 
            // var bufferSize = 2048;
            // recorder = context.createJavaScriptNode(bufferSize, 2, 2);
         
            // recorder.onaudioprocess = function(e){
            //     console.log ('recording');
            //     var left = e.inputBuffer.getChannelData (0);
            //     var right = e.inputBuffer.getChannelData (1);
            //     // we clone the samples
            //     leftchannel.push (new Float32Array (left));
            //     rightchannel.push (new Float32Array (right));
            //     recordingLength += bufferSize;
            // }
         
            // // we connect the recorder
            // volume.connect (recorder);
            // recorder.connect (context.destination); 

        }

        // function interleave(leftChannel, rightChannel){
        //     var length = leftChannel.length + rightChannel.length;
        //     var result = new Float32Array(length);

        //     var inputIndex = 0;

        //     for (var index = 0; index < length; ){
        //     result[index++] = leftChannel[inputIndex];
        //     result[index++] = rightChannel[inputIndex];
        //     inputIndex++;
        //     }
        //     return result;
        // }

        // function writeUTFBytes(view, offset, string){ 
        //     var lng = string.length;
        //     for (var i = 0; i < lng; i++){
        //     view.setUint8(offset + i, string.charCodeAt(i));
        //     }
        // }

        // // we flat the left and right channels down
        // var leftBuffer = mergeBuffers ( leftchannel, recordingLength );
        // var rightBuffer = mergeBuffers ( rightchannel, recordingLength );
        // // we interleave both channels together
        // var interleaved = interleave ( leftBuffer, rightBuffer );
         
        // // create the buffer and view to create the .WAV file
        // var buffer = new ArrayBuffer(44 + interleaved.length * 2);
        // var view = new DataView(buffer);
         
        // // write the WAV container, check spec at: https://ccrma.stanford.edu/courses/422/projects/WaveFormat/
        // // RIFF chunk descriptor
        // writeUTFBytes(view, 0, 'RIFF');
        // view.setUint32(4, 44 + interleaved.length * 2, true);
        // writeUTFBytes(view, 8, 'WAVE');
        // // FMT sub-chunk
        // writeUTFBytes(view, 12, 'fmt ');
        // view.setUint32(16, 16, true);
        // view.setUint16(20, 1, true);
        // // stereo (2 channels)
        // view.setUint16(22, 2, true);
        // view.setUint32(24, sampleRate, true);
        // view.setUint32(28, sampleRate * 4, true);
        // view.setUint16(32, 4, true);
        // view.setUint16(34, 16, true);
        // // data sub-chunk
        // writeUTFBytes(view, 36, 'data');
        // view.setUint32(40, interleaved.length * 2, true);
         
        // // write the PCM samples
        // var lng = interleaved.length;
        // var index = 44;
        // var volume = 1;
        // for (var i = 0; i < lng; i++){
        //     view.setInt16(index, interleaved[i] * (0x7FFF * volume), true);
        //     index += 2;
        // }

        socket.on('broadcast', function(stream){
            var audio = document.querySelector('audio');
            var binarydata = [];
            binarydata.push(stream.data);
            audio.src = window.URL.createObjectURL(new Blob(binarydata));
            audio.onloadedmetadata = function(e){
                audio.play();
            };
            console.log('message arrived');
        });

////////////////////
        function handleError(error) {
            console.log('navigator.getUserMedia error: ', error);
        }

        navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia ||
                navigator.mozGetUserMedia || navigator.msGetUserMedia;

        navigator.getUserMedia(constraints, handleSuccess, handleError);

    </script>
</body>
</html>

